{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aae9160",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59193d17",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c707607",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e503fbf",
   "metadata": {
    "height": 164
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d564ae5",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3afcc69f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75254394",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f405c81",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a1fbec7",
   "metadata": {
    "height": 29
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4e64a54",
   "metadata": {
    "height": 63
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aba4e12e",
   "metadata": {
    "height": 46
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to finetune_fair_diffusion', name='summary_tool_finetune_fair_diffusion', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19d731b4",
   "metadata": {
    "height": 266
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ac3b83f",
   "metadata": {
    "height": 80
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation datasets mentioned in the provided context include PG19, the cleaned Arxiv Math proof-pile dataset, LongBench, LEval, and LongAlpaca-12k. Additionally, the proposed dataset called SAFECONV is designed for evaluating conversation safety.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results demonstrate that the models achieve better perplexity with longer context sizes and show that different attention patterns have varying impacts on model performance during fine-tuning. The models are fine-tuned via the next token prediction objective and achieve promising results on extremely large settings. Additionally, the evaluation on long-context benchmarks highlights the efficiency and effectiveness of the 7B model in achieving comparable or better results within a reasonable training time.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA includes PG19, the cleaned Arxiv Math proof-pile dataset, LongBench, LEval, LongAlpaca-12k, and the proposed dataset called SAFECONV. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes and demonstrate that different attention patterns have varying impacts on model performance during fine-tuning. The models are fine-tuned via the next token prediction objective and show promising results on extremely large settings. The evaluation on long-context benchmarks highlights the efficiency and effectiveness of the 7B model in achieving comparable or better results within a reasonable training time.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05fca7ee",
   "metadata": {
    "height": 80
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the research project includes three benchmarks: HumanEval, MBPP, and SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev includes 70 diverse software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization. The evaluation metrics for HumanEval and MBPP include Pass @ k for functional accuracy, while SoftwareDev evaluation includes metrics like Executability, Cost, Code Statistics, Productivity, and Human Revision Cost. The dataset provides insights into the performance and characteristics of the generated code for different software development tasks, offering a comprehensive view of the software development process using MetaGPT.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset mentioned in the context is designed to test the capabilities of language models in resolving real-world software engineering problems. It consists of task instances sourced from real GitHub issues and pull requests, presenting a wide range of challenges for evaluating language models in the context of software engineering. The dataset includes finalized task instances that have been validated for usability through execution-based validation, ensuring that the solutions and tests can be successfully applied, installed, and run on the codebase. Additionally, the dataset provides information on model-generated patches for resolving software repository issues, which are evaluated based on their effectiveness in addressing the identified problems and passing the required tests.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes three benchmarks: HumanEval, MBPP, and SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev includes 70 diverse software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization. The evaluation metrics for HumanEval and MBPP include Pass @ k for functional accuracy, while SoftwareDev evaluation includes metrics like Executability, Cost, Code Statistics, Productivity, and Human Revision Cost. This dataset provides insights into the performance and characteristics of the generated code for different software development tasks, offering a comprehensive view of the software development process using MetaGPT.\n",
      "\n",
      "On the other hand, the evaluation dataset in SWE-Bench is designed to test the capabilities of language models in resolving real-world software engineering problems. It consists of task instances sourced from real GitHub issues and pull requests, presenting a wide range of challenges for evaluating language models in the context of software engineering. The dataset includes finalized task instances that have been validated for usability through execution-based validation, ensuring that the solutions and tests can be successfully applied, installed, and run on the codebase. Additionally, the dataset provides information on model-generated patches for resolving software repository issues, which are evaluated based on their effectiveness in addressing the identified problems and passing the required tests.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
