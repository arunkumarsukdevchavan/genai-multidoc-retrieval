{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a93ef068",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df66d0fd",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50679b27",
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "# Define URLs and paper names\n",
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "619dc118",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:39,042 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: longlora.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:40,606 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: selfrag.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:42,800 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f968ff4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "184474c5",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d25c3ad6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66f73ad0",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27154734",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:43,744 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:44,057 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:44,465 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "PG19 test split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:45,075 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:45,301 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:46,216 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. By increasing the context window size, the perplexity decreases, indicating the effectiveness of the fine-tuning method. Additionally, the models are extended to handle extremely large context lengths, with promising results, although there is some perplexity degradation on small context sizes for the extended models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:47,392 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "As for the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to a decrease in perplexity, indicating the effectiveness of the fine-tuning method. The models are also extended to handle extremely large context lengths, with promising results. However, there is some perplexity degradation on small context sizes for the extended models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dc151aa",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:48,248 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:48,816 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2025-05-10 05:51:48,817 - INFO - Retrying request to /chat/completions in 0.988401 seconds\n",
      "2025-05-10 05:51:48,894 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:49,168 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:49,669 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:49,686 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:49,813 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:49,872 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:50,336 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:50,404 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:50,973 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:52,153 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It allows a language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This approach enables the language model to tailor its behavior to diverse task requirements, leading to significant performance improvements compared to state-of-the-art models on various tasks such as open-domain QA, reasoning, and fact verification.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:53,048 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:53,261 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:53,441 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:53,619 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:53,624 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:53,714 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:53,736 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:51:54,917 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational resources compared to full fine-tuning. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning to longer context lengths, maintaining performance and reducing memory costs. LongLoRA aims to bridge the performance gap between short and long context lengths, achieving promising results in long-sequence language modeling tasks and retrieval-based evaluations. Additionally, it demonstrates comparable or superior performance to other long-context models like Vicuna and LongChat, while being efficient in terms of training hours and computational overhead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:57,580 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It allows a language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This approach enables the language model to tailor its behavior to diverse task requirements, leading to significant performance improvements compared to state-of-the-art models on various tasks such as open-domain QA, reasoning, and fact verification.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational resources compared to full fine-tuning. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning to longer context lengths, maintaining performance and reducing memory costs. LongLoRA aims to bridge the performance gap between short and long context lengths, achieving promising results in long-sequence language modeling tasks and retrieval-based evaluations. Additionally, it demonstrates comparable or superior performance to other long-context models like Vicuna and LongChat, while being efficient in terms of training hours and computational overhead.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It allows a language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This approach enables the language model to tailor its behavior to diverse task requirements, leading to significant performance improvements compared to state-of-the-art models on various tasks such as open-domain QA, reasoning, and fact verification.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational resources compared to full fine-tuning. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning to longer context lengths, maintaining performance and reducing memory costs. LongLoRA aims to bridge the performance gap between short and long context lengths, achieving promising results in long-sequence language modeling tasks and retrieval-based evaluations. Additionally, it demonstrates comparable or superior performance to other long-context models like Vicuna and LongChat, while being efficient in terms of training hours and computational overhead.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "458c0bfd",
   "metadata": {
    "height": 472
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a1d7fde",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:51:58,719 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: longlora.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:52:00,228 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: loftq.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:52:01,196 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: swebench.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:52:04,548 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: selfrag.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:52:07,003 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: zipformer.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:52:08,051 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: values.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:52:09,909 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: finetune_fair_diffusion.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:52:17,195 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: knowledge_card.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:52:19,117 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metra.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:52:21,044 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:52:23,291 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8854530e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "787c0936",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:54:56,442 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9c7b442",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f46cb57",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:54:58,990 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61b6bc1c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b829afdb",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0abc3099",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:55:02,963 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:03,779 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:55:04,327 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:04,427 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:04,524 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:04,699 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:04,845 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:04,908 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:05,735 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is a combination of HumanEval and MBPP.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:55:06,359 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:06,439 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:06,568 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:06,628 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:06,974 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:07,102 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:07,137 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:07,190 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:07,209 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:07,447 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:07,462 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:07,469 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:07,520 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:08,882 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub issues and corresponding pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files and documentation, an example patch file, and a prompt for generating the patch file. The dataset aims to establish a baseline for approaches to resolving software engineering tasks and encourages experimentation with different methodologies. Additionally, the dataset is constructed by scraping pull requests from the top Python packages in PyPI libraries, filtering through repositories, and converting qualifying PRs into task instances for model evaluation. The dataset is validated for usability through execution-based verification and includes attributes like lines added, lines removed, and various statistics characterizing the task instances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:55:09,228 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:11,297 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is a combination of HumanEval and MBPP. \n",
      "\n",
      "The evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub issues and corresponding pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files and documentation, an example patch file, and a prompt for generating the patch file. The dataset aims to establish a baseline for approaches to resolving software engineering tasks and encourages experimentation with different methodologies. Additionally, the dataset is constructed by scraping pull requests from the top Python packages in PyPI libraries, filtering through repositories, and converting qualifying PRs into task instances for model evaluation. The dataset is validated for usability through execution-based verification and includes attributes like lines added, lines removed, and various statistics characterizing the task instances.\n",
      "assistant: The evaluation dataset used in MetaGPT is a combination of HumanEval and MBPP. \n",
      "\n",
      "The evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub issues and corresponding pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files and documentation, an example patch file, and a prompt for generating the patch file. The dataset aims to establish a baseline for approaches to resolving software engineering tasks and encourages experimentation with different methodologies. Additionally, the dataset is constructed by scraping pull requests from the top Python packages in PyPI libraries, filtering through repositories, and converting qualifying PRs into task instances for model evaluation. The dataset is validated for usability through execution-based verification and includes attributes like lines added, lines removed, and various statistics characterizing the task instances.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cf5b4d6",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:55:11,495 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:55:12,334 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"Compare and contrast the approach in the LongLoRA paper.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:55:13,575 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:14,049 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:14,154 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:14,274 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:14,407 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:14,423 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:14,606 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:16,270 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "The approach in the LongLoRA paper combines Shifted Sparse Attention (S2-Attn) with Low-rank Adaptation (LoRA) to efficiently extend the context window of pre-trained large language models (LLMs) while minimizing computational costs. S2-Attn splits the context length into groups for more efficient attention computation, allowing information flow between neighboring groups. On the other hand, LoRA modifies linear projection layers using low-rank matrices to approximate full fine-tuning. However, plain low-rank adaptation alone is not as effective for training long context models. LongLoRA addresses this by incorporating trainable normalization and embedding layers (LoRA+) to improve adaptation to longer context lengths. This approach distinguishes itself by focusing on efficient fine-tuning, context extension, and maintaining the original architecture integrity during inference, offering a balance between performance and efficiency in handling long-context benchmarks.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"Compare and contrast the approach in the LoftQ paper.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:55:17,553 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:17,819 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:17,829 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:18,141 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:18,727 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:22,046 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "The approach in the LoftQ paper focuses on simultaneously quantizing a Large Language Model (LLM) and finding a suitable low-rank initialization for LoRA fine-tuning. This involves applying Singular Value Decomposition (SVD) to obtain low-rank approximations of pre-trained weight matrices, iteratively refining the quantized weights and low-rank adapters. LoftQ integrates low-rank approximation with quantization to jointly approximate the original high-precision weights, providing an effective initialization for LoRA fine-tuning. During fine-tuning, only the low-rank adapters are optimized while the integer weight matrix is frozen, reducing training costs and showcasing compatibility with different quantization functions.\n",
      "\n",
      "In contrast, the existing approach QLoRA primarily focuses on quantization techniques and may overlook the importance of subsequent LoRA fine-tuning. QLoRA utilizes zero-initialized low-rank adapters attached to the quantized pre-trained model, potentially leading to performance degradation, especially in low-bit scenarios. LoftQ outperforms QLoRA in terms of stability and performance, showcasing its ability to consistently achieve robust results by effectively preserving the starting point of pre-trained weights. LoftQ's method of alternating quantization and low-rank approximation stands out for its ability to narrow the initialization gap and achieve convergence to reasonable results, particularly excelling in low-bit quantization scenarios such as 2-bit precision.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 05:55:22,407 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 05:55:24,924 - INFO - HTTP Request: POST http://jupyter-api-proxy.internal.dlai/rev-proxy/full_power_standard_openai_for_lama_index/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "In the LongLoRA paper, the approach combines Shifted Sparse Attention (S2-Attn) with Low-rank Adaptation (LoRA) to extend the context window of pre-trained large language models efficiently. S2-Attn splits the context length into groups for more efficient attention computation, while LoRA modifies linear projection layers using low-rank matrices to approximate full fine-tuning. LongLoRA incorporates trainable normalization and embedding layers (LoRA+) to improve adaptation to longer context lengths, focusing on efficient fine-tuning, context extension, and maintaining the original architecture integrity during inference.\n",
      "\n",
      "On the other hand, the LoftQ paper focuses on simultaneously quantizing a Large Language Model (LLM) and finding a suitable low-rank initialization for LoRA fine-tuning. It applies Singular Value Decomposition (SVD) to obtain low-rank approximations of pre-trained weight matrices, refining the quantized weights and low-rank adapters iteratively. LoftQ integrates low-rank approximation with quantization to jointly approximate the original high-precision weights, providing an effective initialization for LoRA fine-tuning. During fine-tuning, only the low-rank adapters are optimized while the integer weight matrix is frozen, reducing training costs and showcasing compatibility with different quantization functions.\n",
      "\n",
      "In contrast, LoftQ outperforms the existing approach QLoRA by focusing on stability and performance, effectively preserving the starting point of pre-trained weights. LoftQ's method of alternating quantization and low-rank approximation narrows the initialization gap and achieves convergence to reasonable results, particularly excelling in low-bit quantization scenarios such as 2-bit precision.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
