{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a748b5e1",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02104561",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27310f7",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c7c256",
   "metadata": {
    "height": 164
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f7e984",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80548259",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675722ac",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "879b8a96",
   "metadata": {
    "height": 165
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9d1753b",
   "metadata": {
    "height": 80
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce3e8518",
   "metadata": {
    "height": 62
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It aims to improve response generation by training a single LM to adaptively retrieve passages on-demand, generate text, and reflect on both the retrieved passages and its own generations using special tokens called reflection tokens. This approach allows the LM to control its behavior during the inference phase, tailoring its responses to diverse task requirements. Self-RAG has shown significant performance improvements over state-of-the-art LLMs and retrieval-augmented models on various tasks, particularly excelling in improving factuality and citation accuracy for long-form text generation.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method designed to extend the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for significant computation savings without compromising performance. LongLoRA has been shown to be effective in various tasks and models of different sizes, demonstrating strong empirical results. Additionally, it retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It aims to improve response generation by training a single LM to adaptively retrieve passages on-demand, generate text, and reflect on both the retrieved passages and its own generations using special tokens called reflection tokens. This approach allows the LM to control its behavior during the inference phase, tailoring its responses to diverse task requirements. Self-RAG has shown significant performance improvements over state-of-the-art LLMs and retrieval-augmented models on various tasks, particularly excelling in improving factuality and citation accuracy for long-form text generation.\n",
      "\n",
      "LongLoRA is an efficient method designed to extend the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for significant computation savings without compromising performance. LongLoRA has been shown to be effective in various tasks and models of different sizes, demonstrating strong empirical results. Additionally, it retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It aims to improve response generation by training a single LM to adaptively retrieve passages on-demand, generate text, and reflect on both the retrieved passages and its own generations using special tokens called reflection tokens. This approach allows the LM to control its behavior during the inference phase, tailoring its responses to diverse task requirements. Self-RAG has shown significant performance improvements over state-of-the-art LLMs and retrieval-augmented models on various tasks, particularly excelling in improving factuality and citation accuracy for long-form text generation.\n",
      "\n",
      "LongLoRA is an efficient method designed to extend the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for significant computation savings without compromising performance. LongLoRA has been shown to be effective in various tasks and models of different sizes, demonstrating strong empirical results. Additionally, it retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac87701",
   "metadata": {
    "height": 471
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be60709a",
   "metadata": {
    "height": 164
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fa62c37",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71e5f8e3",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c675879",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9be39d4a",
   "metadata": {
    "height": 63
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a64f65f",
   "metadata": {
    "height": 29
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2608962f",
   "metadata": {
    "height": 266
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61563042",
   "metadata": {
    "height": 97
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval and MBPP, which consist of programming tasks, along with a self-generated software development benchmark called SoftwareDev. The SoftwareDev dataset contains various software development tasks such as mini-games, image processing algorithms, and data visualization.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub repositories, including Python repositories, with a focus on software engineering tasks. The dataset is designed to be challenging and realistic, covering a variety of programming problems and issues commonly encountered in software development. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama were evaluated on this dataset using BM25 and \"oracle\" retrieval settings with different maximum context lengths. The dataset aims to provide a comprehensive environment for evaluating language models in the context of software engineering tasks.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval and MBPP, which consist of programming tasks, along with a self-generated software development benchmark called SoftwareDev. The SoftwareDev dataset contains various software development tasks such as mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub repositories, including Python repositories, with a focus on software engineering tasks. The dataset is designed to be challenging and realistic, covering a variety of programming problems and issues commonly encountered in software development. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama were evaluated on this dataset using BM25 and \"oracle\" retrieval settings with different maximum context lengths. The dataset aims to provide a comprehensive environment for evaluating language models in the context of software engineering tasks.\n",
      "assistant: The evaluation dataset used in MetaGPT includes HumanEval and MBPP, which consist of programming tasks, along with a self-generated software development benchmark called SoftwareDev. The SoftwareDev dataset contains various software development tasks such as mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub repositories, including Python repositories, with a focus on software engineering tasks. The dataset is designed to be challenging and realistic, covering a variety of programming problems and issues commonly encountered in software development. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama were evaluated on this dataset using BM25 and \"oracle\" retrieval settings with different maximum context lengths. The dataset aims to provide a comprehensive environment for evaluating language models in the context of software engineering tasks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d3d0d4e",
   "metadata": {
    "height": 80
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"Approach in LongLoRA\"}\n",
      "=== Function Output ===\n",
      "The approach in LongLoRA involves efficiently extending the context length of large language models (LLMs) to significantly larger sizes while saving on computational costs compared to traditional methods. It combines shifted sparse attention (S2-Attn) during fine-tuning with a parameter-efficient regime, emphasizing the importance of trainable embedding and normalization layers for successful context extension. Additionally, LongLoRA retains the original architecture during inference to save on fine-tuning costs while maintaining the quality of the original attention, setting it apart from other position embedding modification methods for long context extension.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"Approach in LoftQ\"}\n",
      "=== Function Output ===\n",
      "The approach in LoftQ involves integrating quantization and low-rank approximation techniques to approximate high-precision pre-trained weights. This method provides an advantageous initialization for LoRA fine-tuning, addressing performance degradation issues that can occur after quantization. The process alternates between quantization and Singular Value Decomposition (SVD) to optimize the quantized backbone and low-rank adapters, ultimately enhancing generalization in downstream tasks. Additionally, during LoRA fine-tuning, the integer weight matrix is frozen, and only the low-rank adapters are optimized, reducing training costs and improving efficiency.\n",
      "=== LLM Response ===\n",
      "The approach in LongLoRA focuses on efficiently extending the context length of large language models (LLMs) to significantly larger sizes while saving on computational costs compared to traditional methods. It combines shifted sparse attention (S2-Attn) during fine-tuning with a parameter-efficient regime, emphasizing the importance of trainable embedding and normalization layers for successful context extension. LongLoRA retains the original architecture during inference to save on fine-tuning costs while maintaining the quality of the original attention, distinguishing it from other position embedding modification methods for long context extension.\n",
      "\n",
      "On the other hand, the approach in LoftQ integrates quantization and low-rank approximation techniques to approximate high-precision pre-trained weights. This method provides an advantageous initialization for LoRA fine-tuning, addressing performance degradation issues that can occur after quantization. The process alternates between quantization and Singular Value Decomposition (SVD) to optimize the quantized backbone and low-rank adapters, ultimately enhancing generalization in downstream tasks. During LoRA fine-tuning, the integer weight matrix is frozen, and only the low-rank adapters are optimized, reducing training costs and improving efficiency.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
